{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9656817, 2)\n",
      "(8961258, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Loves twitter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg so bored &amp;amp; my tattoooos are so itchy!!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is going to sleep then on a bike ride:]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @SmartChickPDX: Was just told that Nike lay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Category\n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...         0\n",
       "1                                      Loves twitter         0\n",
       "2  omg so bored &amp; my tattoooos are so itchy!!...         0\n",
       "3            is going to sleep then on a bike ride:]         0\n",
       "4  RT @SmartChickPDX: Was just told that Nike lay...         0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "\n",
    "df = pd.concat([pd.read_csv('data/data_{}.csv'.format(a)) for a in range(10)])\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365553"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets 8430264\n",
      "Russian 2283309\n"
     ]
    }
   ],
   "source": [
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "df['Len'] = df['Tweet'].str.split().apply(len)\n",
    "df = df[df['Len']>=4]\n",
    "print('tweets',df.shape[0])\n",
    "print('Russian',np.sum(df['Category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # @somone to @\n",
    "    text = ' '.join(['@' if '@' in a else a for a in text.split(' ')])\n",
    "    # text = ' '.join(['#' if '#' in a else a for a in text.split(' ')])\n",
    "    # remove all punctuation except @ and #\n",
    "    text = ''.join([word.lower() for word in text if word not in string.punctuation.replace('@','').replace('#','')])\n",
    "    # remove numbers\n",
    "    text = re.sub('[0-9]+', 'number', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens2 = [a[0] if '@' in a[0] or '#' in a[0] else a[1] for a in nltk.pos_tag(text.split())]\n",
    "    # hastags only for grammer-check, none in words\n",
    "    tokens = [a.replace('#','') for a in tokens]\n",
    "    tokens2 = ['#' if '#' in a else a for a in tokens2]\n",
    "    lst = []\n",
    "    lst = [ps.stem(word) for word in tokens]\n",
    "    for i in range(4):\n",
    "        for a in nltk.ngrams([ps.stem(word) for word in tokens2],i+3):\n",
    "            lst.append(' '.join(a))\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8430264 Number of tweets has 430 words and took 15971 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "start = time.time()\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text\n",
    "                                  , max_df = .9\n",
    "                                  , min_df = 0.01 \n",
    "                                 )\n",
    "countVector = countVectorizer.fit_transform(df['Tweet'])\n",
    "end = time.time()\n",
    "\n",
    "print('{} Number of tweets has {} words and took {:.0f} s'.format(countVector.shape[0], countVector.shape[1],end-start))\n",
    "# print(countVectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( countVectorizer, open( \"files/countVectorizer.pickle\", \"wb\" ) )\n",
    "pickle.dump( countVector, open( \"files/countVector.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( df, open( \"files/df.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '# NN NN', '@', '@ @ @', '@ @ @ @', '@ @ @ @ @', '@ @ NN', '@ JJ NN', '@ NN @', '@ NN IN', '@ NN NN', '@ NN NN NN', '@ NN RB', '@ NN vbd', '@ NN vbp', '@ nn vbp', 'CC JJ NN', 'DT JJ JJ', 'DT JJ NN', 'DT JJ NN IN', 'DT JJ NN NN', 'DT JJ nn', 'DT NN CC', 'DT NN IN', 'DT NN IN DT', 'DT NN IN DT NN', 'DT NN IN NN', 'DT NN JJ', 'DT NN NN', 'DT NN NN IN', 'DT NN NN NN', 'DT NN RB', 'DT NN TO', 'DT NN vbd', 'DT NN vbz', 'DT nn vbp', 'IN DT JJ', 'IN DT JJ NN', 'IN DT JJ NN NN', 'IN DT NN', 'IN DT NN IN', 'IN DT NN NN', 'IN DT nn', 'IN JJ JJ', 'IN JJ NN', 'IN JJ NN NN', 'IN JJ nn', 'IN NN CC', 'IN NN IN', 'IN NN IN NN', 'IN NN NN', 'IN NN NN NN', 'IN NN RB', 'IN NN nn', 'IN nn vbp', 'IN prp vbp', 'IN prp$ JJ', 'IN prp$ NN', 'IN prp$ NN NN', 'JJ IN NN', 'JJ JJ NN', 'JJ JJ NN NN', 'JJ JJ nn', 'JJ NN #', 'JJ NN @', 'JJ NN CC', 'JJ NN IN', 'JJ NN IN DT', 'JJ NN IN DT NN', 'JJ NN IN JJ', 'JJ NN IN NN', 'JJ NN JJ', 'JJ NN JJ NN', 'JJ NN NN', 'JJ NN NN IN', 'JJ NN NN NN', 'JJ NN NN NN NN', 'JJ NN NN vbp', 'JJ NN RB', 'JJ NN TO', 'JJ NN TO VB', 'JJ NN nn', 'JJ NN nn vbp', 'JJ NN prp', 'JJ NN vbd', 'JJ NN vbp', 'JJ NN vbz', 'JJ TO VB', 'JJ nn IN', 'JJ nn NN', 'JJ nn vbp', 'JJ nnp nnp', 'JJ nnp nnp nnp', 'JJ nnp nnp nnp nnp', 'JJ nnp nnp nnp nnp nnp', 'MD VB DT', 'NN # #', 'NN @ #', 'NN @ @', 'NN @ NN', 'NN @ NN @', 'NN @ NN NN', 'NN CC JJ', 'NN CC NN', 'NN DT NN', 'NN IN DT', 'NN IN DT JJ', 'NN IN DT JJ NN', 'NN IN DT NN', 'NN IN DT NN NN', 'NN IN JJ', 'NN IN JJ NN', 'NN IN JJ NN NN', 'NN IN NN', 'NN IN NN IN', 'NN IN NN NN', 'NN IN NN NN NN', 'NN IN nn', 'NN IN prp', 'NN IN prp$', 'NN IN prp$ NN', 'NN IN vbg', 'NN JJ JJ', 'NN JJ NN', 'NN JJ NN NN', 'NN JJ nn', 'NN MD VB', 'NN NN #', 'NN NN @', 'NN NN CC', 'NN NN DT', 'NN NN IN', 'NN NN IN DT', 'NN NN IN JJ', 'NN NN IN NN', 'NN NN JJ', 'NN NN JJ NN', 'NN NN MD', 'NN NN NN', 'NN NN NN IN', 'NN NN NN JJ', 'NN NN NN NN', 'NN NN NN NN NN', 'NN NN NN vbd', 'NN NN NN vbp', 'NN NN NN vbz', 'NN NN RB', 'NN NN TO', 'NN NN TO VB', 'NN NN VB', 'NN NN nn', 'NN NN nn vbp', 'NN NN prp', 'NN NN vbd', 'NN NN vbg', 'NN NN vbp', 'NN NN vbp NN', 'NN NN vbz', 'NN RB IN', 'NN RB JJ', 'NN RB RB', 'NN RB VB', 'NN RB vbd', 'NN RB vbz', 'NN TO VB', 'NN nn IN', 'NN nn vbp', 'NN nnp nnp', 'NN prp vbd', 'NN prp vbp', 'NN prp$ NN', 'NN vbd DT', 'NN vbd DT NN', 'NN vbd IN', 'NN vbd JJ', 'NN vbd NN', 'NN vbd RB', 'NN vbd prp', 'NN vbn IN', 'NN vbp DT', 'NN vbp IN', 'NN vbp JJ', 'NN vbp NN', 'NN vbp RB', 'NN vbp prp', 'NN vbz DT', 'NN vbz DT NN', 'NN vbz IN', 'NN vbz JJ', 'NN vbz JJ NN', 'NN vbz NN', 'NN vbz RB', 'NN vbz vbg', 'NN vbz vbn', 'RB DT NN', 'RB IN DT', 'RB IN NN', 'RB JJ IN', 'RB JJ NN', 'RB JJ NN NN', 'RB RB JJ', 'TO DT NN', 'TO VB DT', 'TO VB DT NN', 'TO VB IN', 'TO VB JJ', 'TO VB JJ NN', 'TO VB NN', 'TO VB RB', 'TO VB prp', 'VB DT JJ', 'VB DT JJ NN', 'VB DT NN', 'VB DT NN NN', 'VB IN DT', 'VB IN NN', 'VB JJ NN', 'VB NN IN', 'VB NN NN', 'VB TO VB', 'VB prp VB', 'VB prp$ NN', 'a', 'about', 'after', 'all', 'am', 'an', 'and', 'are', 'as', 'at', 'back', 'be', 'been', 'better', 'but', 'by', 'call', 'can', 'cant', 'come', 'day', 'did', 'do', 'dont', 'down', 'even', 'feel', 'follow', 'for', 'from', 'get', 'girl', 'go', 'gon', 'good', 'got', 'great', 'ha', 'had', 'haha', 'happi', 'have', 'he', 'her', 'here', 'hi', 'him', 'home', 'hope', 'how', 'i', 'if', 'ill', 'im', 'in', 'is', 'it', 'just', 'know', 'last', 'let', 'life', 'like', 'lmao', 'lol', 'look', 'love', 'make', 'man', 'me', 'miss', 'more', 'morn', 'much', 'my', 'n', 'na', 'need', 'never', 'new', 'news', 'night', 'nn IN DT', 'nn IN DT NN', 'nn IN JJ', 'nn IN NN', 'nn JJ NN', 'nn TO VB', 'nn vbp DT', 'nn vbp IN', 'nn vbp JJ', 'nn vbp JJ NN', 'nn vbp NN', 'nn vbp RB', 'nnp NN NN', 'nnp nnp NN', 'nnp nnp NN NN', 'nnp nnp nnp', 'nnp nnp nnp NN', 'nnp nnp nnp nnp', 'nnp nnp nnp nnp NN', 'nnp nnp nnp nnp nnp', 'nnp nnp nnp nnp nnp NN', 'nnp nnp nnp nnp nnp nnp', 'no', 'not', 'now', 'number', 'of', 'off', 'oh', 'on', 'one', 'onli', 'or', 'out', 'over', 'peopl', 'play', 'prp MD VB', 'prp vbp JJ', 'prp$ JJ NN', 'prp$ NN IN', 'prp$ NN NN', 'prp$ NN vbz', 'realli', 'right', 'rt', 's', 'say', 'see', 'she', 'shit', 'should', 'show', 'so', 'some', 'start', 'still', 'take', 'thank', 'that', 'the', 'them', 'then', 'there', 'they', 'thi', 'thing', 'think', 'time', 'to', 'today', 'tonight', 'too', 'tri', 'trump', 'twitter', 'u', 'up', 'ur', 'us', 'vbd DT JJ', 'vbd DT JJ NN', 'vbd DT NN', 'vbd JJ NN', 'vbd TO VB', 'vbg DT NN', 'vbg IN NN', 'vbg JJ NN', 'vbg NN NN', 'vbg TO VB', 'vbn IN DT', 'vbn IN NN', 'vbp DT JJ', 'vbp DT NN', 'vbp IN NN', 'vbp JJ NN', 'vbp NN NN', 'vbp TO VB', 'vbz DT JJ', 'vbz DT JJ NN', 'vbz DT NN', 'vbz JJ NN', 'vbz RB JJ', 'wa', 'wait', 'want', 'watch', 'way', 'we', 'well', 'were', 'what', 'when', 'where', 'whi', 'who', 'will', 'with', 'work', 'would', 'year', 'you', 'your', 'в', 'на', '’']\n"
     ]
    }
   ],
   "source": [
    "print(countVectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
